{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Describe the class of strings matched by the following regular expressions.No code is needed and just describe what the following regular expressions do/match).\n",
    "\n",
    "#a.[a-zA-Z]+\n",
    "#b.[A-Z][a-z]*\n",
    "#c.p[aeiou]{,2}t\n",
    "#d.\\d+(\\.\\d+)?\n",
    "#e.([^aeiou][aeiou][^aeiou])*\n",
    "#f.\\w+|[^\\w\\s]+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a. Matches one or more of a range of characters from a-zA-Z\n",
    "#b. Matches zero or more of a range of characters from A-Z and a-z\n",
    "#c. Matches words which have p, followed by one or no more than 2 repeats of characters from a, e, i, o, u, and followed by t\n",
    "#d. Matches words which have one or more decimal digit [0-9], followed by zero or one of a period and zero or one or more decimal digit [0-9]\n",
    "#e. Matches words which have zero or more any character other than a vowel, followed by a vowel, and followed by any character other than a vowel\n",
    "#f. Matches word which have one or more of any alphanumeric character [a-zA-Z0-9_] or which have one or more of any non-alphanumeric character and not any whitespace character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Rewrite the following loop as a list comprehension:\n",
    "# sent = ['This', 'is', 'an', 'introduction', 'class']\n",
    "# result = []\n",
    "# for word in sent:\n",
    "#     word_len = (word, len(word))\n",
    "#     result.append(word_len)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 4), ('is', 2), ('an', 2), ('introduction', 12), ('class', 5)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(word, len(word)) for word in ['This', 'is', 'an', 'introduction', 'class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Read in some text from your own document in your local disk, tokenize it, and print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses and exclamations: who, which, what, and so on.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = open('shakespeare.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"wh'er\",\n",
       " 'whale',\n",
       " 'whales',\n",
       " 'whales-bone',\n",
       " 'wharf',\n",
       " 'wharfs',\n",
       " 'what',\n",
       " \"whate'er\",\n",
       " 'whatever',\n",
       " \"whatsoe'er\",\n",
       " 'whatsoever',\n",
       " 'whe',\n",
       " 'wheat',\n",
       " 'wheaten',\n",
       " 'wheel',\n",
       " 'wheeling',\n",
       " 'wheels',\n",
       " 'wheezing',\n",
       " 'whelk',\n",
       " 'whelks',\n",
       " 'whelm',\n",
       " 'whelp',\n",
       " 'whelped',\n",
       " 'whelps',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whencesoever',\n",
       " \"whene'er\",\n",
       " 'whenever',\n",
       " 'whensoever',\n",
       " 'where',\n",
       " \"where'er\",\n",
       " \"where-e'er\",\n",
       " 'whereabout',\n",
       " 'whereas',\n",
       " 'whereat',\n",
       " 'whereby',\n",
       " 'wherefore',\n",
       " 'wherein',\n",
       " 'whereinto',\n",
       " 'whereof',\n",
       " 'whereon',\n",
       " 'whereout',\n",
       " \"wheresoe'er\",\n",
       " 'wheresoever',\n",
       " \"wheresome'er\",\n",
       " 'whereto',\n",
       " 'whereuntil',\n",
       " 'whereunto',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'wherewith',\n",
       " 'wherewithal',\n",
       " 'whet',\n",
       " \"whet'st\",\n",
       " 'whether',\n",
       " 'whetstone',\n",
       " 'whetted',\n",
       " 'whey',\n",
       " 'wheyface',\n",
       " 'which',\n",
       " 'whiff',\n",
       " 'whiffler',\n",
       " 'while',\n",
       " 'while-ere',\n",
       " 'whiles',\n",
       " 'whilst',\n",
       " 'whin',\n",
       " 'whine',\n",
       " 'whining',\n",
       " 'whip',\n",
       " 'whipp',\n",
       " \"whipp'st\",\n",
       " 'whipped',\n",
       " 'whippers',\n",
       " 'whipping',\n",
       " 'whipping-cheer',\n",
       " 'whips',\n",
       " 'whipster',\n",
       " 'whipstock',\n",
       " 'whipt',\n",
       " 'whirl',\n",
       " 'whirled',\n",
       " 'whirligig',\n",
       " 'whirling',\n",
       " 'whirlpool',\n",
       " 'whirls',\n",
       " 'whirlwind',\n",
       " 'whirlwinds',\n",
       " 'whisper',\n",
       " 'whispered',\n",
       " 'whispering',\n",
       " 'whisperings',\n",
       " 'whispers',\n",
       " 'whist',\n",
       " 'whistle',\n",
       " 'whistles',\n",
       " 'whistling',\n",
       " 'whit',\n",
       " \"whit'st\",\n",
       " 'white',\n",
       " 'white-bearded',\n",
       " 'white-cold',\n",
       " 'white-fac',\n",
       " 'white-lim',\n",
       " 'white-livered',\n",
       " 'white-upturned',\n",
       " 'whiteness',\n",
       " 'whiter',\n",
       " 'whites',\n",
       " 'whitest',\n",
       " 'whither',\n",
       " 'whiting-time',\n",
       " 'whitsters',\n",
       " 'whittle',\n",
       " 'whizzing',\n",
       " 'who',\n",
       " \"whoe'er\",\n",
       " 'whoever',\n",
       " 'whole',\n",
       " \"wholesom'st\",\n",
       " 'wholesome',\n",
       " 'wholesome-profitable',\n",
       " 'wholly',\n",
       " 'whom',\n",
       " 'whoo-bub',\n",
       " 'whoop',\n",
       " 'whooping',\n",
       " 'whor',\n",
       " 'whore',\n",
       " 'whoremaster',\n",
       " 'whoremasterly',\n",
       " 'whoremonger',\n",
       " 'whores',\n",
       " 'whoreson',\n",
       " 'whoresons',\n",
       " 'whoring',\n",
       " 'whorish',\n",
       " 'whose',\n",
       " 'whosesole',\n",
       " 'whoso',\n",
       " \"whosoe'er\",\n",
       " 'whosoever',\n",
       " 'why']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set(w.lower() for w in tokens if w.startswith('wh')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create your own file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g. fuzzy 53. Read the file into a Python list using open(filename).readlines(). Next, break each line into its two fields using split(), and convert the number into an integer using int(). The result should be a list of the form: [['fuzzy', 53], ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('words frequencies.txt').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = [line.split() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fuzzy', 53], ['Python', 22], ['whale', 78], ['whatever', 58], ['wheat', 77]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[split[i][0], int(split[i][1])] for i in range(len(split))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 .Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. Let us define μw to be the average number of letters per word, and μs to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text is defined to be: 4.71 μw + 0.5 μs - 21.43. Compute the ARI score for each section of the Brown Corpus (i.e. News, Editorial,… Humor) (Hint: for category in brown.categories( )). Make use of the fact that nltk.corpus.brown.words() produces a sequence of words, while nltk.corpus.brown.sents() produces a sequence of sentences. (Hint: from nltk.corpus import brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventure 4.0841684990890705\n",
      "belles_lettres 10.987652885621749\n",
      "editorial 9.471025332953673\n",
      "fiction 4.9104735321302115\n",
      "government 12.08430349501021\n",
      "hobbies 8.922356393630267\n",
      "humor 7.887805248319808\n",
      "learned 11.926007043317348\n",
      "lore 10.254756197101155\n",
      "mystery 3.8335518942055167\n",
      "news 10.176684595052684\n",
      "religion 10.203109907301261\n",
      "reviews 10.769699888473433\n",
      "romance 4.34922419804213\n",
      "science_fiction 4.978058336905399\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(brown.categories())):\n",
    "    μw = sum([len(i) for i in brown.words(categories = brown.categories()[n])]) / len(brown.words(categories = brown.categories()[n]))\n",
    "    μs = len(brown.words(categories = brown.categories()[n])) / len(brown.sents(categories = brown.categories()[n]))\n",
    "    ari = 4.71*μw + 0.5*μs - 21.43\n",
    "    print(brown.categories()[n], ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.Use the Porter Stemmer to normalize some tokenized text (see below), calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and describe any difference you observe by using these two stemmers.\n",
    "# text='Technologies based on NLP are becoming increasingly widespread. For example, phones and handheld computers support predictive text and handwriting recognition; web search engines give access to information locked up in unstructured text; machine translation allows us to retrieve texts written in Chinese and read them in Spanish; text analysis enables us to detect sentiment in tweets and blogs. By providing more natural human-machine interfaces, and more sophisticated access to stored information, language processing has come to play a central role in the multilingual information society'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"Technologies based on NLP are becoming increasingly widespread. For example, phones and handheld computers support predictive text and handwriting recognition; web search engines give access to information locked up in unstructured text; machine translation allows us to retrieve texts written in Chinese and read them in Spanish; text analysis enables us to detect sentiment in tweets and blogs. By providing more natural human-machine interfaces, and more sophisticated access to stored information, language processing has come to play a central role in the multilingual information society\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['technolog',\n",
       " 'base',\n",
       " 'on',\n",
       " 'nlp',\n",
       " 'are',\n",
       " 'becom',\n",
       " 'increasingli',\n",
       " 'widespread',\n",
       " '.',\n",
       " 'for',\n",
       " 'exampl',\n",
       " ',',\n",
       " 'phone',\n",
       " 'and',\n",
       " 'handheld',\n",
       " 'comput',\n",
       " 'support',\n",
       " 'predict',\n",
       " 'text',\n",
       " 'and',\n",
       " 'handwrit',\n",
       " 'recognit',\n",
       " ';',\n",
       " 'web',\n",
       " 'search',\n",
       " 'engin',\n",
       " 'give',\n",
       " 'access',\n",
       " 'to',\n",
       " 'inform',\n",
       " 'lock',\n",
       " 'up',\n",
       " 'in',\n",
       " 'unstructur',\n",
       " 'text',\n",
       " ';',\n",
       " 'machin',\n",
       " 'translat',\n",
       " 'allow',\n",
       " 'us',\n",
       " 'to',\n",
       " 'retriev',\n",
       " 'text',\n",
       " 'written',\n",
       " 'in',\n",
       " 'chines',\n",
       " 'and',\n",
       " 'read',\n",
       " 'them',\n",
       " 'in',\n",
       " 'spanish',\n",
       " ';',\n",
       " 'text',\n",
       " 'analysi',\n",
       " 'enabl',\n",
       " 'us',\n",
       " 'to',\n",
       " 'detect',\n",
       " 'sentiment',\n",
       " 'in',\n",
       " 'tweet',\n",
       " 'and',\n",
       " 'blog',\n",
       " '.',\n",
       " 'By',\n",
       " 'provid',\n",
       " 'more',\n",
       " 'natur',\n",
       " 'human-machin',\n",
       " 'interfac',\n",
       " ',',\n",
       " 'and',\n",
       " 'more',\n",
       " 'sophist',\n",
       " 'access',\n",
       " 'to',\n",
       " 'store',\n",
       " 'inform',\n",
       " ',',\n",
       " 'languag',\n",
       " 'process',\n",
       " 'ha',\n",
       " 'come',\n",
       " 'to',\n",
       " 'play',\n",
       " 'a',\n",
       " 'central',\n",
       " 'role',\n",
       " 'in',\n",
       " 'the',\n",
       " 'multilingu',\n",
       " 'inform',\n",
       " 'societi']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[porter.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['technolog',\n",
       " 'bas',\n",
       " 'on',\n",
       " 'nlp',\n",
       " 'ar',\n",
       " 'becom',\n",
       " 'increas',\n",
       " 'widespread',\n",
       " '.',\n",
       " 'for',\n",
       " 'exampl',\n",
       " ',',\n",
       " 'phon',\n",
       " 'and',\n",
       " 'handheld',\n",
       " 'comput',\n",
       " 'support',\n",
       " 'predict',\n",
       " 'text',\n",
       " 'and',\n",
       " 'handwrit',\n",
       " 'recognit',\n",
       " ';',\n",
       " 'web',\n",
       " 'search',\n",
       " 'engin',\n",
       " 'giv',\n",
       " 'access',\n",
       " 'to',\n",
       " 'inform',\n",
       " 'lock',\n",
       " 'up',\n",
       " 'in',\n",
       " 'unstruct',\n",
       " 'text',\n",
       " ';',\n",
       " 'machin',\n",
       " 'transl',\n",
       " 'allow',\n",
       " 'us',\n",
       " 'to',\n",
       " 'retriev',\n",
       " 'text',\n",
       " 'writ',\n",
       " 'in',\n",
       " 'chines',\n",
       " 'and',\n",
       " 'read',\n",
       " 'them',\n",
       " 'in',\n",
       " 'span',\n",
       " ';',\n",
       " 'text',\n",
       " 'analys',\n",
       " 'en',\n",
       " 'us',\n",
       " 'to',\n",
       " 'detect',\n",
       " 'senty',\n",
       " 'in',\n",
       " 'tweet',\n",
       " 'and',\n",
       " 'blog',\n",
       " '.',\n",
       " 'by',\n",
       " 'provid',\n",
       " 'mor',\n",
       " 'nat',\n",
       " 'human-machine',\n",
       " 'interfac',\n",
       " ',',\n",
       " 'and',\n",
       " 'mor',\n",
       " 'soph',\n",
       " 'access',\n",
       " 'to',\n",
       " 'stor',\n",
       " 'inform',\n",
       " ',',\n",
       " 'langu',\n",
       " 'process',\n",
       " 'has',\n",
       " 'com',\n",
       " 'to',\n",
       " 'play',\n",
       " 'a',\n",
       " 'cent',\n",
       " 'rol',\n",
       " 'in',\n",
       " 'the',\n",
       " 'multil',\n",
       " 'inform',\n",
       " 'socy']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Porter and Lancaster stemmers have their own rules to strip affixes. Most of the results are identical or similar, but I found Porter stemmer is more reader-friendly because it still easy to understand the words and their nature while Lancaster's results can be confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Obtain raw texts from two or more genres and compute their respective reading difficulty scores as in the earlier exercise on reading difficulty. Please compare the reading difficulties for ABC Rural News (\"rural.txt\") and ABC Science News(\"science.txt\") (nltk.corpus.abc).(Hint: from nltk.corpus import abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "rural = nltk.corpus.abc.raw('rural.txt')\n",
    "science = nltk.corpus.abc.raw('science.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_rural = word_tokenize(rural)\n",
    "tokens_science = word_tokenize(science)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_rural = nltk.sent_tokenize(rural)\n",
    "sents_science = nltk.sent_tokenize(science)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.616619848644696"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "μw_rural = sum([len(i) for i in tokens_rural]) / len(tokens_rural)\n",
    "μs_rural = len(tokens_rural) / len(sents_rural)\n",
    "4.71*μw_rural + 0.5*μs_rural - 21.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.773321874632714"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "μw_science = sum([len(i) for i in tokens_science]) / len(tokens_science)\n",
    "μs_science = len(tokens_science) / len(sents_science)\n",
    "4.71*μw_science + 0.5*μs_science - 21.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seen from the reading difficuties of two news. We can tell that ABC Rural News is easier to read than Science News, which makes sense, since Science News could use more professional and technical words and terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.Rewrite the following nested loop as a nested list comprehension:\n",
    "#  words = ['attribution', 'confabulation', 'elocution',\n",
    "#         'sequoia', 'tenacious', 'unidirectional']\n",
    "#  vsequences = set()\n",
    "#  for word in words:\n",
    "#      vowels = []\n",
    "#      for char in word:\n",
    "#          if char in 'aeiou':\n",
    "#              vowels.append(char)\n",
    "#      vsequences.add(''.join(vowels))\n",
    "#  sorted(vsequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set(''.join(char for char in word if char in 'aeiou') for word in ['attribution', 'confabulation', 'elocution', 'sequoia', 'tenacious', 'unidirectional']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.Try to refer the following sample code to print the following sentences in a formatted way.(Hint: you should use str.format() method in print() and for loop；For more information, please read the textbook section 3.9 in chapter 3) \n",
    "# output should look like:\n",
    "The Tragedie of Hamlet was written by William Shakespeare in 1599\n",
    "Leaves of Grass        was written by Walt Whiteman       in 1855\n",
    "Emma                   was written by Jane Austen         in 1816\n",
    "# sample code:\n",
    "template = 'Lee wants a {} right now' \n",
    "menu = ['sandwich', 'spam fritter', 'pancake']\n",
    "for snack in menu:\n",
    "    print(template.format(snack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tragedie of Hamlet was written by William Shakespeare in 1599\n",
      "Leaves of Grass        was written by Walt Whiteman       in 1855\n",
      "Emma                   was written by Jane Austen         in 1816\n"
     ]
    }
   ],
   "source": [
    "template = '{:<22} was written by {:<19} in {}'\n",
    "dic = ['The Tragedie of Hamlet', 'William Shakespeare', 1599, 'Leaves of Grass', 'Walt Whiteman', 1855, 'Emma', 'Jane Austen', 1816]\n",
    "for i in range(len(dic)):\n",
    "    if i % 3 == 0:\n",
    "        print(template.format(dic[i], dic[i+1], dic[i+2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Define the variable quote to contain the list ['Action', 'speaks', 'louder', 'than', 'words']. Process this list using a for loop, and store the length of each word in a new list lengths. Hint: begin by assigning the empty list to lengths, using lengths = []. Then each time through the loop, use append() to add another length value to the list. Then do the same thing using a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 6, 6, 4, 5]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['Action', 'speaks', 'louder', 'than', 'words']\n",
    "lengths = []\n",
    "for word in words:\n",
    "    lengths.append(len(word))\n",
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 6, 6, 4, 5]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(word) for word in ['Action', 'speaks', 'louder', 'than', 'words']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
